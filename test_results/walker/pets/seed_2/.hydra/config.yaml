seed: 2
device: cuda:0
log_frequency_agent: 1000
save_video: false
debug_mode: false
render: false
experiment: pets_final_tests
root_dir: ./exp
algorithm:
  name: pets
  agent:
    _target_: mbrl.planning.TrajectoryOptimizerAgent
    action_lb: ???
    action_ub: ???
    planning_horizon: ${overrides.planning_horizon}
    optimizer_cfg: ${action_optimizer}
    replan_freq: 1
    verbose: ${debug_mode}
  normalize: true
  normalize_double_precision: true
  target_is_delta: true
  initial_exploration_steps: ${overrides.trial_length}
  freq_train_model: ${overrides.freq_train_model}
  learned_rewards: ${overrides.learned_rewards}
  num_particles: 20
dynamics_model:
  _target_: mbrl.models.GaussianMLP
  device: ${device}
  num_layers: 4
  in_size: ???
  out_size: ???
  ensemble_size: 5
  hid_size: 200
  deterministic: false
  propagation_method: random_model
  learn_logvar_bounds: false
  activation_fn_cfg:
    _target_: torch.nn.SiLU
overrides:
  env: gym___Walker2d-v2
  term_fn: walker2d
  reward_fn: walker2d
  learned_rewards: false
  num_steps: 200000
  trial_length: 1000
  agent_type: Exploration
  freq_model_checkpoint: 10000
  num_uncertainty_trajectories: 3
  random_uncertainty: true
  num_elites: 5
  model_lr: 0.0004
  model_wd: 0.00017
  model_batch_size: 32
  validation_ratio: 0.05
  no_delta_list:
  - 0
  freq_train_model: 1000
  patience: 25
  num_epochs_train_model: 15
  planning_horizon: 15
  cem_num_iters: 5
  cem_elite_ratio: 0.1
  cem_population_size: 250
  cem_alpha: 0.1
  cem_clipped_normal: false
action_optimizer:
  _target_: mbrl.planning.CEMOptimizer
  num_iterations: ${overrides.cem_num_iters}
  elite_ratio: ${overrides.cem_elite_ratio}
  population_size: ${overrides.cem_population_size}
  alpha: ${overrides.cem_alpha}
  lower_bound: ???
  upper_bound: ???
  return_mean_elites: true
  device: ${device}
  clipped_normal: ${overrides.cem_clipped_normal}
